# configs/base_config.yaml
# Base configuration for DDP throughput benchmark

model:
  name: "gpt2"            # Which pretrained model to load (GPT‑2 small ~124M)
  max_length: 512          # Sequence length per sample (attention is O(n^2))
  
dataset:
  name: "wikitext"        # HuggingFace dataset name
  subset: "wikitext-2-raw-v1"  # Which subset/split variant to use
  max_samples: 1000        # Cap training samples for quick local dev
  
training:
  batch_size: 4            # Per‑device (per‑GPU) batch size
  global_batch_size: 32    # Target effective batch across GPUs *and* grad accumulation
  gradient_accumulation_steps: 8  # Steps to accumulate grads before an optimizer step
  max_steps: 50           # Total training steps (use this instead of a hard‑coded 3)
  learning_rate: 5e-5      # AdamW LR
  warmup_steps: 10         # LR warmup steps before linear decay
  precision: "fp16"       # Switch to "fp16"/"bf16" later for speed on GPU

# [EXPLAIN] On Mac you’ll use gloo/mps; on NVIDIA Linux boxes use nccl.
distributed:
  backend: "nccl"         # Will use gloo on Mac; nccl on CUDA machines
  find_unused_parameters: false

logging:
  log_level: "INFO"
  log_dir: "experiments/logs"    # Where to write metrics / profiler traces
  wandb_project: "ddp-throughput-benchmark"

metrics:
  measure_gpu_util: true          # Enable later when on real GPU
  measure_throughput: true        # We’ll compute tokens/sec per step
  profile_steps: [10, 20, 30]     # Step indices to wrap with torch.profiler (DDP phase)