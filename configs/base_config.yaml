# configs/base_config.yaml — AWS p3.8xlarge tuned

model:
  name: "gpt2"              # 124M
  max_length: 512            # tokenized sequence length

dataset:
  name: "wikitext"
  subset: "wikitext-2-raw-v1"
  max_samples: 1000          # keep small for throughput bench
  num_workers: 8             # per process; tune 4–8
  prefetch_factor: 2

training:
  batch_size: 4              # per-GPU micro-batch
  global_batch_size: 32      # across all GPUs *after* grad accum
  gradient_accumulation_steps: 8
  max_steps: 100             # short runs for clean timing
  learning_rate: 5e-5
  warmup_steps: 10
  save_steps: 0              # disable to avoid I/O noise during bench
  eval_steps: 0              # disable mid-run eval for timing purity
  precision: "fp16"         # V100 supports FP16 (bf16 not native)
  deterministic: false

distributed:
  backend: "nccl"
  find_unused_parameters: false

logging:
  log_level: "INFO"
  log_dir: "experiments/logs"
  wandb_project: "ddp-throughput-benchmark"

metrics:
  measure_gpu_util: true
  measure_throughput: true
  profile_steps: [10, 20, 30]  # short profile windows

paths:
  datasets_cache: "~/datasets"
  hf_cache: "~/cache/hf"
  torch_cache: "~/cache/torch"